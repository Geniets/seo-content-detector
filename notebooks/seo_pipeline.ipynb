{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SEO Content Quality & Duplicate Detector\n",
        "\n",
        "This notebook implements a machine learning pipeline for:\n",
        "- HTML content parsing and text extraction\n",
        "- Feature engineering (readability, keywords, embeddings)\n",
        "- Duplicate detection using cosine similarity\n",
        "- Content quality classification\n",
        "- Real-time URL analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From d:\\Anaconda\\envs\\main\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n",
            "All libraries imported successfully!\n",
            "All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import warnings\n",
        "import json\n",
        "import joblib\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "import nltk\n",
        "import textstat\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "print(\"All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Collection & HTML Parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded: 81 rows\n",
            "\n",
            "Columns: ['url', 'html_content']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>html_content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.cm-alliance.com/cybersecurity-blog</td>\n",
              "      <td>&lt;!doctype html&gt;&lt;!--[if lt IE 7]&gt; &lt;html class=\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.varonis.com/blog/cybersecurity-tips</td>\n",
              "      <td>&lt;!doctype html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;\\n    &lt;me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://www.cisecurity.org/insights/blog/11-cy...</td>\n",
              "      <td>&lt;!DOCTYPE html&gt;&lt;html data-unhead-vue-server-re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://www.cisa.gov/topics/cybersecurity-best...</td>\n",
              "      <td>\\n\\n&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en\" dir=\"ltr\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://www.qnbtrust.bank/Resources/Learning-C...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 url  \\\n",
              "0     https://www.cm-alliance.com/cybersecurity-blog   \n",
              "1    https://www.varonis.com/blog/cybersecurity-tips   \n",
              "2  https://www.cisecurity.org/insights/blog/11-cy...   \n",
              "3  https://www.cisa.gov/topics/cybersecurity-best...   \n",
              "4  https://www.qnbtrust.bank/Resources/Learning-C...   \n",
              "\n",
              "                                        html_content  \n",
              "0  <!doctype html><!--[if lt IE 7]> <html class=\"...  \n",
              "1  <!doctype html><html lang=\"en\"><head>\\n    <me...  \n",
              "2  <!DOCTYPE html><html data-unhead-vue-server-re...  \n",
              "3  \\n\\n<!DOCTYPE html>\\n<html lang=\"en\" dir=\"ltr\"...  \n",
              "4                                                NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "data_path = Path('../data/data.csv')\n",
        "\n",
        "if not data_path.exists():\n",
        "    print(\"Dataset not found at data/data.csv\")\n",
        "else:\n",
        "    df = pd.read_csv(data_path)\n",
        "    print(f\"Dataset loaded: {len(df)} rows\")\n",
        "    print(f\"\\nColumns: {df.columns.tolist()}\")\n",
        "    display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HTML parsing function defined\n"
          ]
        }
      ],
      "source": [
        "def parse_html_content(html_content):\n",
        "    try:\n",
        "        soup = BeautifulSoup(html_content, 'lxml')\n",
        "        \n",
        "        title = soup.find('title')\n",
        "        title = title.get_text().strip() if title else 'No Title'\n",
        "        \n",
        "        for script in soup([\"script\", \"style\", \"meta\", \"link\", \"noscript\"]):\n",
        "            script.decompose()\n",
        "        \n",
        "        main_content = soup.find('main') or soup.find('article') or soup.find('body')\n",
        "        \n",
        "        if main_content:\n",
        "            text_elements = main_content.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li'])\n",
        "            body_text = ' '.join([elem.get_text().strip() for elem in text_elements])\n",
        "        else:\n",
        "            body_text = soup.get_text()\n",
        "        \n",
        "        body_text = re.sub(r'\\s+', ' ', body_text).strip()\n",
        "        body_text = re.sub(r'[^\\w\\s.,!?;:\\'-]', '', body_text)\n",
        "        \n",
        "        word_count = len(body_text.split())\n",
        "        \n",
        "        return {\n",
        "            'title': title,\n",
        "            'body_text': body_text,\n",
        "            'word_count': word_count\n",
        "        }\n",
        "    \n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'title': 'Error',\n",
        "            'body_text': '',\n",
        "            'word_count': 0\n",
        "        }\n",
        "\n",
        "print(\"HTML parsing function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parsing HTML content...\n",
            "Processed 10/81 pages\n",
            "Processed 10/81 pages\n",
            "Processed 20/81 pages\n",
            "Processed 20/81 pages\n",
            "Processed 30/81 pages\n",
            "Processed 30/81 pages\n",
            "Processed 40/81 pages\n",
            "Processed 40/81 pages\n",
            "Processed 50/81 pages\n",
            "Processed 50/81 pages\n",
            "Processed 60/81 pages\n",
            "Processed 60/81 pages\n",
            "Processed 70/81 pages\n",
            "Processed 70/81 pages\n",
            "Processed 80/81 pages\n",
            "Processed 80/81 pages\n",
            "\n",
            "Extracted content from 68 pages\n",
            "\n",
            "Extracted content from 68 pages\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "      <th>body_text</th>\n",
              "      <th>word_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.cm-alliance.com/cybersecurity-blog</td>\n",
              "      <td>Cyber Security Blog</td>\n",
              "      <td>Back Training NCSC Assured Cyber Incident Plan...</td>\n",
              "      <td>1358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.varonis.com/blog/cybersecurity-tips</td>\n",
              "      <td>Top 10 Cybersecurity Awareness Tips: How to St...</td>\n",
              "      <td>Top 10 Cybersecurity Awareness Tips: How to St...</td>\n",
              "      <td>1664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://www.cisecurity.org/insights/blog/11-cy...</td>\n",
              "      <td>11 Cyber Defense Tips to Stay Secure at Work a...</td>\n",
              "      <td>11 Cyber Defense Tips to Stay Secure at Work a...</td>\n",
              "      <td>1034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://www.cisa.gov/topics/cybersecurity-best...</td>\n",
              "      <td>Cybersecurity Best Practices | Cybersecurity a...</td>\n",
              "      <td>Cybersecurity Best Practices Cybersecurity Bes...</td>\n",
              "      <td>630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>https://nordlayer.com/learn/network-security/b...</td>\n",
              "      <td>Network Security 101: Understanding the Basics</td>\n",
              "      <td>Home Learning center Network security Network ...</td>\n",
              "      <td>2130</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 url  \\\n",
              "0     https://www.cm-alliance.com/cybersecurity-blog   \n",
              "1    https://www.varonis.com/blog/cybersecurity-tips   \n",
              "2  https://www.cisecurity.org/insights/blog/11-cy...   \n",
              "3  https://www.cisa.gov/topics/cybersecurity-best...   \n",
              "5  https://nordlayer.com/learn/network-security/b...   \n",
              "\n",
              "                                               title  \\\n",
              "0                                Cyber Security Blog   \n",
              "1  Top 10 Cybersecurity Awareness Tips: How to St...   \n",
              "2  11 Cyber Defense Tips to Stay Secure at Work a...   \n",
              "3  Cybersecurity Best Practices | Cybersecurity a...   \n",
              "5     Network Security 101: Understanding the Basics   \n",
              "\n",
              "                                           body_text  word_count  \n",
              "0  Back Training NCSC Assured Cyber Incident Plan...        1358  \n",
              "1  Top 10 Cybersecurity Awareness Tips: How to St...        1664  \n",
              "2  11 Cyber Defense Tips to Stay Secure at Work a...        1034  \n",
              "3  Cybersecurity Best Practices Cybersecurity Bes...         630  \n",
              "5  Home Learning center Network security Network ...        2130  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"Parsing HTML content...\")\n",
        "\n",
        "parsed_data = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    url = row['url']\n",
        "    \n",
        "    if 'html_content' in df.columns:\n",
        "        html_content = row['html_content']\n",
        "        parsed = parse_html_content(html_content)\n",
        "    else:\n",
        "        print(f\"No html_content column. Scraping {url}...\")\n",
        "        try:\n",
        "            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
        "            response = requests.get(url, headers=headers, timeout=10)\n",
        "            html_content = response.text\n",
        "            parsed = parse_html_content(html_content)\n",
        "            time.sleep(1)\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping {url}: {e}\")\n",
        "            parsed = {'title': 'Error', 'body_text': '', 'word_count': 0}\n",
        "    \n",
        "    parsed_data.append({\n",
        "        'url': url,\n",
        "        'title': parsed['title'],\n",
        "        'body_text': parsed['body_text'],\n",
        "        'word_count': parsed['word_count']\n",
        "    })\n",
        "    \n",
        "    if (idx + 1) % 10 == 0:\n",
        "        print(f\"Processed {idx + 1}/{len(df)} pages\")\n",
        "\n",
        "extracted_df = pd.DataFrame(parsed_data)\n",
        "extracted_df = extracted_df[extracted_df['word_count'] > 0]\n",
        "extracted_df.to_csv('../data/extracted_content.csv', index=False)\n",
        "\n",
        "print(f\"\\nExtracted content from {len(extracted_df)} pages\")\n",
        "display(extracted_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting features...\n",
            "Basic features extracted\n",
            "\n",
            "Loading sentence transformer model...\n",
            "Basic features extracted\n",
            "\n",
            "Loading sentence transformer model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf0acd421b0144c7b70db1e4bc7975f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f58efabd87b442d5905667fd80436b8a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4d4791e303e4667913f45c54cdb9f42",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77ddf4d7ca1f4e0ba1bb2d903289a8b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1e83b76455d47a7b6b2144a5e8d6e6d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "413f962385a34ceeaafdb1d8d9983b51",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5fc6de37be74e9dae1d4d82abbcc25e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf3daa2048524fd69a855b1ca8617f17",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff5fcb93386b4a04bf2a9ed5a31e1258",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42b34e5b60304e7bb7bcad4419e11625",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e9e0ee81e7f452bb787b7a409e552cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating embeddings...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "498d20dbb5da4541b6fbee2125a5ac20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "All features extracted\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>word_count</th>\n",
              "      <th>sentence_count</th>\n",
              "      <th>flesch_reading_ease</th>\n",
              "      <th>top_keywords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.cm-alliance.com/cybersecurity-blog</td>\n",
              "      <td>1358</td>\n",
              "      <td>17</td>\n",
              "      <td>-61.200375</td>\n",
              "      <td>cyber|cybersecurity|events|training|tabletop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.varonis.com/blog/cybersecurity-tips</td>\n",
              "      <td>1664</td>\n",
              "      <td>91</td>\n",
              "      <td>41.140144</td>\n",
              "      <td>data|access|security|information|users</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://www.cisecurity.org/insights/blog/11-cy...</td>\n",
              "      <td>1034</td>\n",
              "      <td>72</td>\n",
              "      <td>53.688621</td>\n",
              "      <td>use|password|data|authentication|email</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://www.cisa.gov/topics/cybersecurity-best...</td>\n",
              "      <td>630</td>\n",
              "      <td>25</td>\n",
              "      <td>-0.238602</td>\n",
              "      <td>cybersecurity|cyber|cisa|practices|best</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>https://nordlayer.com/learn/network-security/b...</td>\n",
              "      <td>2130</td>\n",
              "      <td>181</td>\n",
              "      <td>26.044653</td>\n",
              "      <td>network|security|access|data|devices</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 url  word_count  \\\n",
              "0     https://www.cm-alliance.com/cybersecurity-blog        1358   \n",
              "1    https://www.varonis.com/blog/cybersecurity-tips        1664   \n",
              "2  https://www.cisecurity.org/insights/blog/11-cy...        1034   \n",
              "3  https://www.cisa.gov/topics/cybersecurity-best...         630   \n",
              "5  https://nordlayer.com/learn/network-security/b...        2130   \n",
              "\n",
              "   sentence_count  flesch_reading_ease  \\\n",
              "0              17           -61.200375   \n",
              "1              91            41.140144   \n",
              "2              72            53.688621   \n",
              "3              25            -0.238602   \n",
              "5             181            26.044653   \n",
              "\n",
              "                                   top_keywords  \n",
              "0  cyber|cybersecurity|events|training|tabletop  \n",
              "1        data|access|security|information|users  \n",
              "2        use|password|data|authentication|email  \n",
              "3       cybersecurity|cyber|cisa|practices|best  \n",
              "5          network|security|access|data|devices  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"Extracting features...\")\n",
        "\n",
        "extracted_df['clean_text'] = extracted_df['body_text'].apply(clean_text)\n",
        "extracted_df['sentence_count'] = extracted_df['body_text'].apply(count_sentences)\n",
        "extracted_df['flesch_reading_ease'] = extracted_df['body_text'].apply(get_readability_score)\n",
        "extracted_df['top_keywords'] = extracted_df['body_text'].apply(extract_top_keywords)\n",
        "\n",
        "print(\"Basic features extracted\")\n",
        "print(\"\\nLoading sentence transformer model...\")\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "print(\"Generating embeddings...\")\n",
        "embeddings = model.encode(extracted_df['clean_text'].tolist(), show_progress_bar=True)\n",
        "extracted_df['embedding'] = [emb.tolist() for emb in embeddings]\n",
        "\n",
        "print(\"\\nAll features extracted\")\n",
        "display(extracted_df[['url', 'word_count', 'sentence_count', 'flesch_reading_ease', 'top_keywords']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature extraction functions defined\n"
          ]
        }
      ],
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def count_sentences(text):\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    return len([s for s in sentences if s.strip()])\n",
        "\n",
        "def get_readability_score(text):\n",
        "    try:\n",
        "        return textstat.flesch_reading_ease(text)\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def extract_top_keywords(text, n=5):\n",
        "    try:\n",
        "        vectorizer = TfidfVectorizer(max_features=n, stop_words='english')\n",
        "        tfidf_matrix = vectorizer.fit_transform([text])\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "        scores = tfidf_matrix.toarray()[0]\n",
        "        keyword_scores = list(zip(feature_names, scores))\n",
        "        keyword_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "        keywords = [kw for kw, score in keyword_scores[:n]]\n",
        "        return '|'.join(keywords)\n",
        "    except:\n",
        "        return ''\n",
        "\n",
        "print(\"Feature extraction functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Text Preprocessing & Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features saved to: data/features.csv\n",
            "\n",
            "Feature Statistics:\n",
            "         word_count  sentence_count  flesch_reading_ease\n",
            "count     68.000000       68.000000            68.000000\n",
            "mean    3235.941176      238.529412            30.177379\n",
            "std     5101.721488      525.458619            36.080936\n",
            "min        8.000000        1.000000          -175.110311\n",
            "25%      566.750000       27.000000            19.080185\n",
            "50%     1636.500000      102.500000            34.732160\n",
            "75%     3425.000000      212.000000            49.819167\n",
            "max    31380.000000     3766.000000           103.540000\n"
          ]
        }
      ],
      "source": [
        "features_df = extracted_df[['url', 'word_count', 'sentence_count', 'flesch_reading_ease', 'top_keywords', 'embedding']].copy()\n",
        "features_df.to_csv('../data/features.csv', index=False)\n",
        "\n",
        "print(\"Features saved to: data/features.csv\")\n",
        "print(\"\\nFeature Statistics:\")\n",
        "print(features_df[['word_count', 'sentence_count', 'flesch_reading_ease']].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def baseline_classifier(word_count):\n",
        "    if word_count > 1500:\n",
        "        return 'High'\n",
        "    elif word_count < 500:\n",
        "        return 'Low'\n",
        "    else:\n",
        "        return 'Medium'\n",
        "\n",
        "baseline_pred = X_test['word_count'].apply(baseline_classifier)\n",
        "baseline_accuracy = accuracy_score(y_test, baseline_pred)\n",
        "\n",
        "print(f\"Baseline Model (Rule-based):\")\n",
        "print(f\"Accuracy: {baseline_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_columns = ['word_count', 'sentence_count', 'flesch_reading_ease']\n",
        "X = extracted_df[feature_columns]\n",
        "y = extracted_df['quality_label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")\n",
        "print(f\"\\nTraining label distribution:\")\n",
        "print(y_train.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def assign_quality_label(row):\n",
        "    word_count = row['word_count']\n",
        "    readability = row['flesch_reading_ease']\n",
        "    \n",
        "    if word_count > 1500 and 50 <= readability <= 70:\n",
        "        return 'High'\n",
        "    elif word_count < 500 or readability < 30:\n",
        "        return 'Low'\n",
        "    else:\n",
        "        return 'Medium'\n",
        "\n",
        "extracted_df['quality_label'] = extracted_df.apply(assign_quality_label, axis=1)\n",
        "\n",
        "print(\"Quality Label Distribution:\")\n",
        "print(extracted_df['quality_label'].value_counts())\n",
        "print(f\"\\nPercentages:\")\n",
        "print(extracted_df['quality_label'].value_counts(normalize=True) * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Content Quality Scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "THIN_CONTENT_THRESHOLD = 500\n",
        "\n",
        "extracted_df['is_thin'] = extracted_df['word_count'] < THIN_CONTENT_THRESHOLD\n",
        "thin_content_count = extracted_df['is_thin'].sum()\n",
        "thin_content_percentage = (thin_content_count / len(extracted_df)) * 100\n",
        "\n",
        "print(f\"\\nContent Analysis Summary:\")\n",
        "print(f\"Total pages analyzed: {len(extracted_df)}\")\n",
        "print(f\"Duplicate pairs found: {len(duplicate_pairs)}\")\n",
        "print(f\"Thin content pages: {thin_content_count} ({thin_content_percentage:.1f}%)\")\n",
        "print(f\"Average word count: {extracted_df['word_count'].mean():.0f}\")\n",
        "print(f\"Average readability score: {extracted_df['flesch_reading_ease'].mean():.1f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Computing cosine similarity matrix...\")\n",
        "\n",
        "embeddings_array = np.array(extracted_df['embedding'].tolist())\n",
        "similarity_matrix = cosine_similarity(embeddings_array)\n",
        "\n",
        "print(f\"Similarity matrix shape: {similarity_matrix.shape}\")\n",
        "\n",
        "SIMILARITY_THRESHOLD = 0.80\n",
        "\n",
        "duplicate_pairs = []\n",
        "\n",
        "for i in range(len(similarity_matrix)):\n",
        "    for j in range(i + 1, len(similarity_matrix)):\n",
        "        similarity = similarity_matrix[i][j]\n",
        "        if similarity > SIMILARITY_THRESHOLD:\n",
        "            duplicate_pairs.append({\n",
        "                'url1': extracted_df.iloc[i]['url'],\n",
        "                'url2': extracted_df.iloc[j]['url'],\n",
        "                'similarity': round(similarity, 4)\n",
        "            })\n",
        "\n",
        "duplicates_df = pd.DataFrame(duplicate_pairs)\n",
        "\n",
        "print(f\"\\nFound {len(duplicate_pairs)} duplicate pairs (similarity > {SIMILARITY_THRESHOLD})\")\n",
        "\n",
        "if len(duplicate_pairs) > 0:\n",
        "    display(duplicates_df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Duplicate Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'duplicates_df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mduplicates_df\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/duplicates.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDuplicates saved to: data/duplicates.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'duplicates_df' is not defined"
          ]
        }
      ],
      "source": [
        "duplicates_df.to_csv('../data/duplicates.csv', index=False)\n",
        "print(\"Duplicates saved to: data/duplicates.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Random Forest Classifier...\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[18], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Random Forest Classifier...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(\n\u001b[0;32m      4\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m      5\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m      6\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[0;32m      7\u001b[0m     class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m )\n\u001b[1;32m---> 10\u001b[0m rf_model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[0;32m     12\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m rf_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     14\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"Training Random Forest Classifier...\")\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    random_state=42,\n",
        "    class_weight='balanced'\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(\"\\nModel trained successfully!\")\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(f\"Baseline Accuracy: {baseline_accuracy:.4f}\")\n",
        "print(f\"Improvement: {(accuracy - baseline_accuracy):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Real-time analysis function defined\n"
          ]
        }
      ],
      "source": [
        "def analyze_url(url, existing_embeddings=None, existing_urls=None):\n",
        "    try:\n",
        "        print(f\"Fetching {url}...\")\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        html_content = response.text\n",
        "        \n",
        "        parsed = parse_html_content(html_content)\n",
        "        \n",
        "        if parsed['word_count'] == 0:\n",
        "            return {'url': url, 'error': 'Failed to extract content'}\n",
        "        \n",
        "        clean_text_content = clean_text(parsed['body_text'])\n",
        "        sentence_count = count_sentences(parsed['body_text'])\n",
        "        readability = get_readability_score(parsed['body_text'])\n",
        "        keywords = extract_top_keywords(parsed['body_text'])\n",
        "        \n",
        "        embedding = model.encode([clean_text_content])[0]\n",
        "        \n",
        "        features = pd.DataFrame([{\n",
        "            'word_count': parsed['word_count'],\n",
        "            'sentence_count': sentence_count,\n",
        "            'flesch_reading_ease': readability\n",
        "        }])\n",
        "        \n",
        "        quality_label = rf_model.predict(features)[0]\n",
        "        \n",
        "        is_thin = parsed['word_count'] < THIN_CONTENT_THRESHOLD\n",
        "        \n",
        "        similar_content = []\n",
        "        if existing_embeddings is not None and existing_urls is not None:\n",
        "            similarities = cosine_similarity([embedding], existing_embeddings)[0]\n",
        "            for i, sim in enumerate(similarities):\n",
        "                if sim > SIMILARITY_THRESHOLD:\n",
        "                    similar_content.append({\n",
        "                        'url': existing_urls[i],\n",
        "                        'similarity': round(float(sim), 4)\n",
        "                    })\n",
        "            similar_content.sort(key=lambda x: x['similarity'], reverse=True)\n",
        "        \n",
        "        return {\n",
        "            'url': url,\n",
        "            'title': parsed['title'],\n",
        "            'word_count': parsed['word_count'],\n",
        "            'sentence_count': sentence_count,\n",
        "            'readability': round(readability, 2),\n",
        "            'top_keywords': keywords,\n",
        "            'quality_label': quality_label,\n",
        "            'is_thin': bool(is_thin),\n",
        "            'similar_to': similar_content[:5]\n",
        "        }\n",
        "    \n",
        "    except Exception as e:\n",
        "        return {'url': url, 'error': str(e)}\n",
        "\n",
        "print(\"Real-time analysis function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "SEO CONTENT QUALITY & DUPLICATE DETECTOR - SUMMARY\n",
            "============================================================\n",
            "\n",
            "Dataset Statistics:\n",
            "Total pages: 68\n",
            "Avg word count: 3236\n",
            "Avg readability: 30.2\n",
            "\n",
            "Duplicate Detection:\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'SIMILARITY_THRESHOLD' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[17], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvg readability: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextracted_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflesch_reading_ease\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDuplicate Detection:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimilarity threshold: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mSIMILARITY_THRESHOLD\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDuplicate pairs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(duplicate_pairs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThin content (<\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTHIN_CONTENT_THRESHOLD\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m words): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthin_content_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthin_content_percentage\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'SIMILARITY_THRESHOLD' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"SEO CONTENT QUALITY & DUPLICATE DETECTOR - SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nDataset Statistics:\")\n",
        "print(f\"Total pages: {len(extracted_df)}\")\n",
        "print(f\"Avg word count: {extracted_df['word_count'].mean():.0f}\")\n",
        "print(f\"Avg readability: {extracted_df['flesch_reading_ease'].mean():.1f}\")\n",
        "\n",
        "print(\"\\nDuplicate Detection:\")\n",
        "print(f\"Similarity threshold: {SIMILARITY_THRESHOLD}\")\n",
        "print(f\"Duplicate pairs: {len(duplicate_pairs)}\")\n",
        "print(f\"Thin content (<{THIN_CONTENT_THRESHOLD} words): {thin_content_count} ({thin_content_percentage:.1f}%)\")\n",
        "\n",
        "print(\"\\nQuality Classification:\")\n",
        "print(f\"Model: Random Forest\")\n",
        "print(f\"Features: {', '.join(feature_columns)}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(f\"Improvement over baseline: {(accuracy - baseline_accuracy):.4f}\")\n",
        "\n",
        "print(\"\\nTop Features:\")\n",
        "for idx, row in feature_importance.iterrows():\n",
        "    print(f\"  {idx+1}. {row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "print(\"\\nOutput Files:\")\n",
        "print(\"  - data/extracted_content.csv\")\n",
        "print(\"  - data/features.csv\")\n",
        "print(\"  - data/duplicates.csv\")\n",
        "print(\"  - models/quality_model.pkl\")\n",
        "\n",
        "print(\"\\nAnalysis complete!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary and Final Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quality label distribution pie chart\n",
        "quality_counts = extracted_df['quality_label'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(quality_counts.values, labels=quality_counts.index, autopct='%1.1f%%',\n",
        "        colors=['#ff9999', '#66b3ff', '#99ff99'], startangle=90)\n",
        "plt.title('Content Quality Distribution')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Similarity heatmap (for a subset of pages)\n",
        "n_samples = min(20, len(similarity_matrix))\n",
        "subset_similarity = similarity_matrix[:n_samples, :n_samples]\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(subset_similarity, cmap='YlOrRd', square=True, \n",
        "            xticklabels=False, yticklabels=False,\n",
        "            cbar_kws={'label': 'Cosine Similarity'})\n",
        "plt.title(f'Content Similarity Heatmap (First {n_samples} Pages)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Word count distribution by quality\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "for label in ['Low', 'Medium', 'High']:\n",
        "    data = extracted_df[extracted_df['quality_label'] == label]['word_count']\n",
        "    plt.hist(data, alpha=0.6, label=label, bins=20)\n",
        "plt.xlabel('Word Count')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Word Count Distribution by Quality')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "for label in ['Low', 'Medium', 'High']:\n",
        "    data = extracted_df[extracted_df['quality_label'] == label]['flesch_reading_ease']\n",
        "    plt.hist(data, alpha=0.6, label=label, bins=20)\n",
        "plt.xlabel('Flesch Reading Ease Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Readability Distribution by Quality')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualizations (Bonus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Testing real-time URL analysis...\\n\")\n",
        "\n",
        "existing_embeddings = embeddings_array\n",
        "existing_urls = extracted_df['url'].tolist()\n",
        "\n",
        "test_url = extracted_df.iloc[0]['url']\n",
        "\n",
        "result = analyze_url(test_url, existing_embeddings, existing_urls)\n",
        "\n",
        "print(json.dumps(result, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Real-Time URL Analysis Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = Path('../models/quality_model.pkl')\n",
        "joblib.dump(rf_model, model_path)\n",
        "\n",
        "model_info = {\n",
        "    'rf_model': 'quality_model.pkl',\n",
        "    'sentence_transformer': 'all-MiniLM-L6-v2',\n",
        "    'feature_columns': feature_columns,\n",
        "    'similarity_threshold': SIMILARITY_THRESHOLD,\n",
        "    'thin_content_threshold': THIN_CONTENT_THRESHOLD\n",
        "}\n",
        "\n",
        "with open('../models/model_info.json', 'w') as f:\n",
        "    json.dump(model_info, f, indent=2)\n",
        "\n",
        "print(f\"Model saved to: {model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_importance = pd.DataFrame({\n",
        "    'feature': feature_columns,\n",
        "    'importance': rf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop Features:\")\n",
        "for idx, row in feature_importance.iterrows():\n",
        "    print(f\"{row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance['feature'], feature_importance['importance'])\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Feature Importance - Quality Classification')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['High', 'Low', 'Medium'],\n",
        "            yticklabels=['High', 'Low', 'Medium'])\n",
        "plt.title('Confusion Matrix - Quality Classification')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
